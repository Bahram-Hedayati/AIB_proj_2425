{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10713730,"sourceType":"datasetVersion","datasetId":6640665}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import keras\n\nfrom keras import layers\nfrom keras import ops\nimport tensorflow as tf\nimport numpy as np\nimport imageio\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:33:32.586355Z","iopub.execute_input":"2025-02-10T15:33:32.586576Z","iopub.status.idle":"2025-02-10T15:33:45.516830Z","shell.execute_reply.started":"2025-02-10T15:33:32.586555Z","shell.execute_reply":"2025-02-10T15:33:45.515921Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"batch_size = 4\nnum_channels = 1\nnum_classes = 5\nimage_size = 512\nlatent_dim = 256 # TUNE\n\ndata_dir = \"/kaggle/input/lung-ds/Full_slice/train\" ###\n\ngenerator_in_channels = latent_dim + num_classes\ndiscriminator_in_channels = num_channels + num_classes\n\ndatagen = ImageDataGenerator(\n    rescale=1./255\n)\n\nimage_generator = datagen.flow_from_directory(\n    data_dir,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    color_mode=\"grayscale\",\n    seed=42            \n)\n\nimages, labels = next(image_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:54:58.697734Z","iopub.execute_input":"2025-02-10T15:54:58.698010Z","iopub.status.idle":"2025-02-10T15:55:00.800229Z","shell.execute_reply.started":"2025-02-10T15:54:58.697990Z","shell.execute_reply":"2025-02-10T15:55:00.799485Z"}},"outputs":[{"name":"stdout","text":"Found 1890 images belonging to 5 classes.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"output_dir = \"/kaggle/working\"\n\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, generator):\n        super().__init__()\n        self.generator = generator\n       \n    def on_epoch_end(self, epoch, logs=None):\n        global latent_dim, num_classes\n        sample_latent = keras.random.normal(shape=(1, latent_dim), seed=42)\n        sample_label = tf.one_hot([2], depth=num_classes)  # assuming class 2\n        sample_input = tf.concat([sample_latent, sample_label], axis=1)\n\n        generated_image = self.generator(sample_input, training=False)\n        generated_image = (generated_image + 1.0) * 127.5\n        generated_image = tf.cast(generated_image, tf.uint8).numpy()[0, :, :, 0]\n\n        imageio.imwrite(f\"{output_dir}/output_epoch_{epoch}.png\", generated_image)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:21:38.948116Z","iopub.execute_input":"2025-02-10T16:21:38.948407Z","iopub.status.idle":"2025-02-10T16:21:38.954876Z","shell.execute_reply.started":"2025-02-10T16:21:38.948386Z","shell.execute_reply":"2025-02-10T16:21:38.954031Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Create the discriminator.\ndiscriminator = keras.Sequential(\n    [\n        keras.layers.InputLayer((512, 512, discriminator_in_channels)),\n        #layers.Conv2D(16, (4, 4), strides=(2, 2), padding=\"same\"), \n        #layers.LeakyReLU(negative_slope=0.2),\n        #layers.Dropout(0.4),\n        \n        layers.Conv2D(64, kernel_size=4, strides=2, padding='same'),\n        layers.LeakyReLU(alpha=0.2),\n        #layers.Dropout(0.3),\n\n        layers.Conv2D(128, kernel_size=4, strides=2, padding='same'),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(alpha=0.2),\n        #layers.Dropout(0.3),\n\n        layers.Conv2D(256, kernel_size=4, strides=2, padding='same'),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(alpha=0.2),\n        #layers.Dropout(0.3),\n        \n        layers.Conv2D(512, kernel_size=4, strides=2, padding='same'),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(alpha=0.2),\n        #layers.Dropout(0.3),\n\n        layers.Conv2D(1024, kernel_size=4, strides=2, padding='same'),\n        layers.BatchNormalization(),\n        layers.LeakyReLU(alpha=0.2),\n        #layers.Dropout(0.3),\n\n        layers.Flatten(),\n        #layers.Dense(128, activation='relu'), # NOW\n        layers.Dense(1, activation='sigmoid')\n    ],\n    name=\"discriminator\",\n)\n\n# Create the generator.\ngenerator = keras.Sequential(\n    [\n        #keras.layers.InputLayer((generator_in_channels,)),\n        \n        # Increase feature map size\n        layers.Dense(16 * 16 * 256, input_dim=generator_in_channels, use_bias=False),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        layers.Reshape((16, 16, 256)),\n\n        # Upsample progressively to 512x512\n        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        \n        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        \n        layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        \n        layers.Conv2DTranspose(16, kernel_size=4, strides=2, padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n\n        layers.Conv2DTranspose(8, kernel_size=4, strides=2, padding='same', use_bias=False),\n        layers.BatchNormalization(),\n        layers.ReLU(),\n        \n        layers.Conv2DTranspose(1, kernel_size=3, strides=1, padding='same', activation='tanh')\n    ],\n    name=\"generator\",\n)\n\nclass ConditionalGAN(keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super().__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n        self.seed_generator = keras.random.SeedGenerator(42)\n        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n\n    @property\n    def metrics(self):\n        return [self.gen_loss_tracker, self.disc_loss_tracker]\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super().compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n\n    def train_step(self, data):\n        global n_epoch, output_dir\n        # Unpack the data.\n        real_images, one_hot_labels = data\n\n        # Add dummy dimensions to the labels so that they can be concatenated with\n        # the images. This is for the discriminator.\n        image_one_hot_labels = one_hot_labels[:, :, None, None]\n        image_one_hot_labels = ops.repeat(\n            image_one_hot_labels, repeats=[image_size * image_size]\n        )\n        image_one_hot_labels = ops.reshape(\n            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n        )\n\n        # Sample random points in the latent space and concatenate the labels.\n        # This is for the generator.\n        batch_size = ops.shape(real_images)[0]\n        random_latent_vectors = keras.random.normal(\n            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n        )\n        random_vector_labels = ops.concatenate(\n            [random_latent_vectors, one_hot_labels], axis=1\n        )\n\n        # Decode the noise (guided by labels) to fake images.\n        generated_images = self.generator(random_vector_labels)\n\n        # Combine them with real images. Note that we are concatenating the labels\n        # with these images here.\n        fake_image_and_labels = ops.concatenate(\n            [generated_images, image_one_hot_labels], -1\n        )\n        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)\n        combined_images = ops.concatenate(\n            [fake_image_and_labels, real_image_and_labels], axis=0\n        )\n\n        # Assemble labels discriminating real from fake images.\n        labels = ops.concatenate(\n            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n        )\n\n        # Train the discriminator.\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        # Sample random points in the latent space.\n        random_latent_vectors = keras.random.normal(\n            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n        )\n        random_vector_labels = ops.concatenate(\n            [random_latent_vectors, one_hot_labels], axis=1\n        )\n\n        # Assemble labels that say \"all real images\".\n        misleading_labels = ops.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n            fake_images = self.generator(random_vector_labels)\n            fake_image_and_labels = ops.concatenate(\n                [fake_images, image_one_hot_labels], -1\n            )\n            predictions = self.discriminator(fake_image_and_labels)\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n        \n        # Monitor loss.\n        self.gen_loss_tracker.update_state(g_loss)\n        self.disc_loss_tracker.update_state(d_loss)\n        return {\n            \"g_loss\": self.gen_loss_tracker.result(),\n            \"d_loss\": self.disc_loss_tracker.result(),\n        }\n\n\ncond_gan = ConditionalGAN(\n    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n)\n\nprint_callback = CustomCallback(generator=generator)\n\ninitial_lr_generator = 0.0002\nlr_schedule_generator = ExponentialDecay(\n    initial_learning_rate=initial_lr_generator,\n    decay_steps=750,    # Adjust based on your training iterations\n    decay_rate=0.96,\n    staircase=True\n)\ngenerator_optimizer = keras.optimizers.Adam(learning_rate=initial_lr_generator, beta_1=0.5)\n\ninitial_lr_discriminator = 0.0001\nlr_schedule_discriminator = ExponentialDecay(\n    initial_learning_rate=initial_lr_discriminator,\n    decay_steps=750,    # Adjust as needed\n    decay_rate=0.96,\n    staircase=True\n)\ndiscriminator_optimizer = keras.optimizers.Adam(learning_rate=initial_lr_discriminator, beta_1=0.5)\n\n\ncond_gan.compile(\n    d_optimizer=discriminator_optimizer,\n    g_optimizer=generator_optimizer,\n    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n)\n\ncond_gan.fit(image_generator, epochs=200, callbacks=[print_callback])\n\ntrained_gen = cond_gan.generator\n\n# Choose the number of intermediate images that would be generated in\n# between the interpolation + 2 (start and last images).\nnum_interpolation = 9  # @param {type:\"integer\"}\n\n# Sample noise for the interpolation.\ninterpolation_noise = keras.random.normal(shape=(1, latent_dim))\ninterpolation_noise = ops.repeat(interpolation_noise, repeats=num_interpolation)\ninterpolation_noise = ops.reshape(interpolation_noise, (num_interpolation, latent_dim))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:22:24.810266Z","iopub.execute_input":"2025-02-10T16:22:24.810592Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 126ms/step - d_loss: 0.3491 - g_loss: 3.2003\nEpoch 2/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.3492 - g_loss: 3.0416\nEpoch 3/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1704 - g_loss: 3.6422\nEpoch 4/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1722 - g_loss: 4.1231\nEpoch 5/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1297 - g_loss: 4.4670\nEpoch 6/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1469 - g_loss: 3.6616\nEpoch 7/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1818 - g_loss: 3.8173\nEpoch 8/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.6251 - g_loss: 4.1780\nEpoch 9/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1713 - g_loss: 3.7079\nEpoch 10/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1919 - g_loss: 3.8371\nEpoch 11/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1774 - g_loss: 4.0718\nEpoch 12/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1882 - g_loss: 3.9084\nEpoch 13/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1683 - g_loss: 3.9340\nEpoch 14/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1552 - g_loss: 4.1645\nEpoch 15/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1478 - g_loss: 4.2170\nEpoch 16/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1327 - g_loss: 4.4924\nEpoch 17/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1324 - g_loss: 4.5202\nEpoch 18/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1270 - g_loss: 4.4516\nEpoch 19/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1253 - g_loss: 4.8193\nEpoch 20/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1211 - g_loss: 4.8623\nEpoch 21/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1033 - g_loss: 5.1192\nEpoch 22/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1079 - g_loss: 5.3364\nEpoch 23/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0994 - g_loss: 5.3784\nEpoch 24/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1221 - g_loss: 5.5257\nEpoch 25/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1157 - g_loss: 5.0780\nEpoch 26/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0926 - g_loss: 5.6655\nEpoch 27/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0933 - g_loss: 5.4359\nEpoch 28/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0887 - g_loss: 5.8137\nEpoch 29/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1017 - g_loss: 6.2053\nEpoch 30/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0822 - g_loss: 5.6063\nEpoch 31/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0643 - g_loss: 6.0556\nEpoch 32/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.1135 - g_loss: 6.4093\nEpoch 33/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1109 - g_loss: 5.6171\nEpoch 34/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0638 - g_loss: 6.6030\nEpoch 35/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0621 - g_loss: 6.0483\nEpoch 36/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0765 - g_loss: 6.6560\nEpoch 37/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0708 - g_loss: 6.5783\nEpoch 38/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0723 - g_loss: 6.8964\nEpoch 39/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0638 - g_loss: 6.8122\nEpoch 40/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0579 - g_loss: 6.9412\nEpoch 41/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0750 - g_loss: 7.4704\nEpoch 42/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0751 - g_loss: 7.0350\nEpoch 43/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0559 - g_loss: 7.3824\nEpoch 44/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0862 - g_loss: 7.6814\nEpoch 45/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0546 - g_loss: 7.9570\nEpoch 46/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0591 - g_loss: 7.7425\nEpoch 47/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0496 - g_loss: 7.4751\nEpoch 48/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0847 - g_loss: 8.4606\nEpoch 49/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0446 - g_loss: 7.8753\nEpoch 50/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0586 - g_loss: 8.5175\nEpoch 51/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0608 - g_loss: 8.4392\nEpoch 52/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0434 - g_loss: 7.9589\nEpoch 53/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1812 - g_loss: 8.7869\nEpoch 54/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0529 - g_loss: 8.7634\nEpoch 55/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0346 - g_loss: 8.7545\nEpoch 56/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0461 - g_loss: 9.1126\nEpoch 57/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0500 - g_loss: 8.8944\nEpoch 58/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0502 - g_loss: 9.4710\nEpoch 59/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0527 - g_loss: 8.4324\nEpoch 60/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0360 - g_loss: 8.3735\nEpoch 61/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0446 - g_loss: 8.9948\nEpoch 62/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0446 - g_loss: 9.5449\nEpoch 63/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0448 - g_loss: 9.7815\nEpoch 64/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0415 - g_loss: 8.8698\nEpoch 65/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0329 - g_loss: 9.8103\nEpoch 66/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0313 - g_loss: 9.5648\nEpoch 67/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0484 - g_loss: 9.8357\nEpoch 68/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0491 - g_loss: 9.5555\nEpoch 69/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.1145 - g_loss: 9.4402\nEpoch 70/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0357 - g_loss: 9.8137\nEpoch 71/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0413 - g_loss: 10.3961\nEpoch 72/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0299 - g_loss: 10.7131\nEpoch 73/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0387 - g_loss: 9.7512\nEpoch 74/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0289 - g_loss: 9.8587\nEpoch 75/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0338 - g_loss: 10.2268\nEpoch 76/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0388 - g_loss: 9.9377\nEpoch 77/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0439 - g_loss: 10.5393\nEpoch 78/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0564 - g_loss: 10.3460\nEpoch 79/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0324 - g_loss: 10.5636\nEpoch 80/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0329 - g_loss: 10.5355\nEpoch 81/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0354 - g_loss: 10.4637\nEpoch 82/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0435 - g_loss: 11.2182\nEpoch 83/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 117ms/step - d_loss: 0.0270 - g_loss: 10.9449\nEpoch 84/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0330 - g_loss: 11.4313\nEpoch 85/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0363 - g_loss: 11.4177\nEpoch 86/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0453 - g_loss: 10.8136\nEpoch 87/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0314 - g_loss: 11.0127\nEpoch 88/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0226 - g_loss: 11.3980\nEpoch 89/200\n\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 116ms/step - d_loss: 0.0382 - g_loss: 12.3248\nEpoch 90/200\n\u001b[1m  6/473\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 117ms/step - d_loss: 0.0024 - g_loss: 9.0868    ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def interpolate_class(first_number, second_number):\n    # Convert the start and end labels to one-hot encoded vectors.\n    first_label = keras.utils.to_categorical([first_number], num_classes)\n    second_label = keras.utils.to_categorical([second_number], num_classes)\n    first_label = ops.cast(first_label, \"float32\")\n    second_label = ops.cast(second_label, \"float32\")\n\n    # Calculate the interpolation vector between the two labels.\n    percent_second_label = ops.linspace(0, 1, num_interpolation)[:, None]\n    percent_second_label = ops.cast(percent_second_label, \"float32\")\n    interpolation_labels = (\n        first_label * (1 - percent_second_label) + second_label * percent_second_label\n    )\n\n    # Combine the noise and the labels and run inference with the generator.\n    noise_and_labels = ops.concatenate([interpolation_noise, interpolation_labels], 1)\n    fake = trained_gen.predict(noise_and_labels)\n    return fake\n\n\nstart_class = 0  # @param {type:\"slider\", min:0, max:9, step:1}\nend_class = 4  # @param {type:\"slider\", min:0, max:9, step:1}\n\nfake_images = interpolate_class(start_class, end_class)\n\nfake_images *= 255.0\nconverted_images = fake_images.astype(np.uint8)\nconverted_images = ops.image.resize(converted_images, (512, 512)).numpy().astype(np.uint8)\nimageio.mimsave(\"animation.gif\", converted_images[:, :, :, 0], fps=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:12:44.982084Z","iopub.execute_input":"2025-02-10T09:12:44.982374Z","iopub.status.idle":"2025-02-10T09:12:46.885789Z","shell.execute_reply.started":"2025-02-10T09:12:44.982351Z","shell.execute_reply":"2025-02-10T09:12:46.884991Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738ms/step\n","output_type":"stream"}],"execution_count":4}]}